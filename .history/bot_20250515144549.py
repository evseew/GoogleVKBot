import sys
import os
import time as time_module
import asyncio
import logging
import datetime
import glob
import io
from io import BytesIO
from collections import defaultdict
from typing import Optional
from datetime import datetime, timedelta
import pytz
import shutil
import requests
import json
import random
import re
import threading

# --- Dependency Imports ---
import vk_api
from vk_api.bot_longpoll import VkBotLongPoll, VkBotEvent, VkBotEventType
from vk_api.utils import get_random_id

import openai
import chromadb
from dotenv import load_dotenv
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload
import docx
import PyPDF2

# LangChain components for specific tasks
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter

# --- Load Environment Variables ---
load_dotenv()

# --- Configuration ---
# VK API Settings
VK_GROUP_TOKEN = os.getenv("VK_GROUP_TOKEN")
VK_GROUP_ID = os.getenv("VK_GROUP_ID")
VK_API_VERSION = os.getenv("VK_API_VERSION", "5.199")

# OpenAI Settings
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
ASSISTANT_ID = os.getenv("ASSISTANT_ID")
EMBEDDING_MODEL = "text-embedding-3-large"
EMBEDDING_DIMENSIONS = 1536

# Google Drive Settings
SERVICE_ACCOUNT_FILE = os.getenv("GOOGLE_SERVICE_ACCOUNT_FILE", 'service-account-key.json')
FOLDER_ID = os.getenv("GOOGLE_DRIVE_FOLDER_ID")

# User/Manager IDs
try:
    ADMIN_USER_ID = int(os.getenv("ADMIN_USER_ID"))
except (TypeError, ValueError):
    raise ValueError("‚ùå –û—à–∏–±–∫–∞: ADMIN_USER_ID –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —á–∏—Å–ª–æ–º –≤ .env!")

try:
    raw_manager_ids = os.getenv("MANAGER_USER_IDS", "").split(',')
    MANAGER_USER_IDS = [int(id_str) for id_str in raw_manager_ids if id_str.strip()]
except ValueError:
     raise ValueError("‚ùå –û—à–∏–±–∫–∞: MANAGER_USER_IDS –≤ .env –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —á–∏—Å–ª–∞–º–∏, —Ä–∞–∑–¥–µ–ª–µ–Ω–Ω—ã–º–∏ –∑–∞–ø—è—Ç—ã–º–∏!")

# Vector Store Settings
VECTOR_DB_BASE_PATH = "./local_vector_db"
ACTIVE_DB_INFO_FILE = "active_db_info.txt"
VECTOR_DB_COLLECTION_NAME = "documents_collection"
RELEVANT_CONTEXT_COUNT = 3

# Bot Behavior Settings
MESSAGE_COOLDOWN_SECONDS = 3
MESSAGE_BUFFER_SECONDS = 4
LOG_RETENTION_SECONDS = 86400
OPENAI_RUN_TIMEOUT_SECONDS = 90

# Time Settings
TIMEZONE_STR = os.getenv("TIMEZONE_STR", "Asia/Yekaterinburg")
WORK_START_HHMM = os.getenv("WORK_START_HHMM", "09:45")
WORK_END_HHMM = os.getenv("WORK_END_HHMM", "19:15")

try:
    TARGET_TZ = pytz.timezone(TIMEZONE_STR)
except pytz.UnknownTimeZoneError:
    logging.error(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —á–∞—Å–æ–≤–æ–π –ø–æ—è—Å '{TIMEZONE_STR}' –≤ .env. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è UTC.") # –ò—Å–ø–æ–ª—å–∑—É–µ–º logging –≤–º–µ—Å—Ç–æ logger, —Ç.–∫. logger –µ—â–µ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω
    TARGET_TZ = pytz.utc

def parse_hhmm(time_str: str, default_time: datetime.time) -> datetime.time:
    try:
        hour, minute = map(int, time_str.split(':'))
        return datetime.time(hour, minute)
    except (ValueError, TypeError):
        logging.error(f"–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –≤—Ä–µ–º–µ–Ω–∏ '{time_str}' –≤ .env. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {default_time.strftime('%H:%M')}.")
        return default_time

WORK_START_TIME = parse_hhmm(WORK_START_HHMM, datetime.time(9, 45))
WORK_END_TIME = parse_hhmm(WORK_END_HHMM, datetime.time(19, 15))

# Commands
CMD_SPEAK = "speak" # –ö–æ–º–∞–Ω–¥–∞ –¥–ª—è —Å–Ω—è—Ç–∏—è –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–≥–æ –º–æ–ª—á–∞–Ω–∏—è

# Logging Settings
LOGS_DIR = "./logs/context_logs"
os.makedirs(LOGS_DIR, exist_ok=True)
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# --- Validate Configuration ---
required_vars = {
    "VK_GROUP_TOKEN": VK_GROUP_TOKEN,
    "VK_GROUP_ID": VK_GROUP_ID,
    "OPENAI_API_KEY": OPENAI_API_KEY,
    "ASSISTANT_ID": ASSISTANT_ID,
    "FOLDER_ID": FOLDER_ID,
    "ADMIN_USER_ID": ADMIN_USER_ID
}
missing_vars = [name for name, value in required_vars.items() if not value]
if missing_vars:
    raise ValueError(f"‚ùå –û—à–∏–±–∫–∞: –ù–µ –Ω–∞–π–¥–µ–Ω—ã –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –≤ .env: {', '.join(missing_vars)}")

# --- Global State (In-Memory) ---
user_threads: dict[str, str] = {}
user_processing_locks: defaultdict[int, asyncio.Lock] = defaultdict(asyncio.Lock)
user_last_message_time: dict[int, datetime] = {}
chat_silence_state: dict[int, bool] = {} # {peer_id: True if silent by CRM}
MY_PENDING_RANDOM_IDS = set()

pending_messages: dict[int, list[str]] = {}
user_message_timers: dict[int, asyncio.Task] = {}

# –§–∞–π–ª –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–≥–æ –º–æ–ª—á–∞–Ω–∏—è
SILENCE_STATE_FILE = "silence_state.json"

# --- Initialize API Clients ---
try:
    openai_client = openai.AsyncOpenAI(api_key=OPENAI_API_KEY)
    logger.info("–ö–ª–∏–µ–Ω—Ç OpenAI –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω.")
except Exception as e:
    logger.critical(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–ª–∏–µ–Ω—Ç OpenAI: {e}", exc_info=True)
    sys.exit(1)

try:
    vk_session_api = vk_api.VkApi(token=VK_GROUP_TOKEN, api_version=VK_API_VERSION)
    logger.info("VK API —Å–µ—Å—Å–∏—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ (–°–ò–ù–•–†–û–ù–ù–û). Long Poll –±—É–¥–µ—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –≤ —Å–≤–æ–µ–º –ø–æ—Ç–æ–∫–µ.")
except vk_api.AuthError as e:
     logger.critical(f"–û—à–∏–±–∫–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ VK: {e}. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ç–æ–∫–µ–Ω –≥—Ä—É–ø–ø—ã.", exc_info=True)
     sys.exit(1)
except Exception as e:
    logger.critical(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ VK API: {e}", exc_info=True)
    sys.exit(1)

vector_collection: Optional[chromadb.api.models.Collection.Collection] = None # –ò—Å–ø–æ–ª—å–∑—É–µ–º Optional

def _get_active_db_subpath() -> Optional[str]: # –ò—Å–ø–æ–ª—å–∑—É–µ–º Optional
    try:
        active_db_info_filepath = os.path.join(VECTOR_DB_BASE_PATH, ACTIVE_DB_INFO_FILE)
        if os.path.exists(active_db_info_filepath):
            with open(active_db_info_filepath, "r", encoding="utf-8") as f:
                active_subdir = f.read().strip()
            if active_subdir:
                if os.path.isdir(os.path.join(VECTOR_DB_BASE_PATH, active_subdir)):
                    logger.info(f"–ù–∞–π–¥–µ–Ω–∞ –∞–∫—Ç–∏–≤–Ω–∞—è –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –ë–î: '{active_subdir}'")
                    return active_subdir
                else:
                    logger.warning(f"–í —Ñ–∞–π–ª–µ '{ACTIVE_DB_INFO_FILE}' —É–∫–∞–∑–∞–Ω–∞ –Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: '{active_subdir}'")
                    return None    
            else:
                logger.warning(f"–§–∞–π–ª '{ACTIVE_DB_INFO_FILE}' –ø—É—Å—Ç.")
                return None
        else:
            logger.info(f"–§–∞–π–ª –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –∞–∫—Ç–∏–≤–Ω–æ–π –ë–î '{ACTIVE_DB_INFO_FILE}' –Ω–µ –Ω–∞–π–¥–µ–Ω.")
            return None
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –∞–∫—Ç–∏–≤–Ω–æ–π –ë–î: {e}", exc_info=True)
        return None

async def _initialize_active_vector_collection():
    global vector_collection
    active_subdir = _get_active_db_subpath()
    if active_subdir:
        active_db_full_path = os.path.join(VECTOR_DB_BASE_PATH, active_subdir)
        try:
            os.makedirs(VECTOR_DB_BASE_PATH, exist_ok=True)
            os.makedirs(active_db_full_path, exist_ok=True) 
            
            chroma_client_init = chromadb.PersistentClient(path=active_db_full_path)
            vector_collection = chroma_client_init.get_or_create_collection(
                name=VECTOR_DB_COLLECTION_NAME,
            )
            logger.info(f"–£—Å–ø–µ—à–Ω–æ –ø–æ–¥–∫–ª—é—á–µ–Ω–æ –∫ ChromaDB: '{active_db_full_path}'. –ö–æ–ª–ª–µ–∫—Ü–∏—è: '{VECTOR_DB_COLLECTION_NAME}'.")
            if vector_collection: # –î–æ–±–∞–≤–ª–µ–Ω–∞ –ø—Ä–æ–≤–µ—Ä–∫–∞
                logger.info(f"–î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∞–∫—Ç–∏–≤–Ω–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ: {vector_collection.count()}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ ChromaDB –¥–ª—è –ø—É—Ç–∏ '{active_db_full_path}': {e}. –ü–æ–∏—Å–∫ –ø–æ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –±—É–¥–µ—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω.", exc_info=True)
            vector_collection = None
    else:
        logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∞–∫—Ç–∏–≤–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ë–î. –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –±—É–¥–µ—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞.")
        vector_collection = None

def get_drive_service():
    try:
        credentials = service_account.Credentials.from_service_account_file(
            SERVICE_ACCOUNT_FILE,
            scopes=['https://www.googleapis.com/auth/drive.readonly']
        )
        service = build('drive', 'v3', credentials=credentials)
        logger.info("–°–µ—Ä–≤–∏—Å Google Drive –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω.")
        return service
    except FileNotFoundError:
        logger.error(f"–§–∞–π–ª –∫–ª—é—á–∞ Google Service Account –Ω–µ –Ω–∞–π–¥–µ–Ω: {SERVICE_ACCOUNT_FILE}")
        return None
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–µ—Ä–≤–∏—Å–∞ Google Drive: {e}", exc_info=True)
        return None

drive_service = get_drive_service()

# --- Helper Functions ---
def get_user_key(user_id: int) -> str:
    return str(user_id)

def is_non_working_hours() -> bool:
    now_local = datetime.now(TARGET_TZ)
    current_time_local = now_local.time()
    is_non_working = current_time_local >= WORK_END_TIME or current_time_local < WORK_START_TIME
    return is_non_working

async def send_vk_message(peer_id: int, message: str):
    if not message:
        logger.warning(f"–ü–æ–ø—ã—Ç–∫–∞ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –ø—É—Å—Ç–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ peer_id={peer_id}")
        return
    try:
        current_random_id = vk_api.utils.get_random_id()
        MY_PENDING_RANDOM_IDS.add(current_random_id)
        logger.debug(f"–î–æ–±–∞–≤–ª–µ–Ω random_id {current_random_id} –≤ MY_PENDING_RANDOM_IDS –¥–ª—è peer_id={peer_id}")

        await asyncio.to_thread(
            vk_session_api.method,
            'messages.send',
            {
                'peer_id': peer_id,
                'message': message,
                'random_id': current_random_id
            }
        )
    except vk_api.exceptions.ApiError as e:
        logger.error(f"–û—à–∏–±–∫–∞ VK API –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ peer_id={peer_id}: {e}", exc_info=True)
        if 'current_random_id' in locals() and current_random_id in MY_PENDING_RANDOM_IDS:
            MY_PENDING_RANDOM_IDS.remove(current_random_id)
            logger.debug(f"–£–¥–∞–ª–µ–Ω random_id {current_random_id} –∏–∑ MY_PENDING_RANDOM_IDS –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏ –æ—Ç–ø—Ä–∞–≤–∫–∏ –¥–ª—è peer_id={peer_id}")
    except Exception as e:
        logger.error(f"–ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ peer_id={peer_id}: {e}", exc_info=True)
        if 'current_random_id' in locals() and current_random_id in MY_PENDING_RANDOM_IDS:
            MY_PENDING_RANDOM_IDS.remove(current_random_id)
            logger.debug(f"–£–¥–∞–ª–µ–Ω random_id {current_random_id} –∏–∑ MY_PENDING_RANDOM_IDS –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏ –æ—Ç–ø—Ä–∞–≤–∫–∏ –¥–ª—è peer_id={peer_id}")

async def set_typing_activity(peer_id: int):
     try:
        await asyncio.to_thread(
             vk_session_api.method,
             'messages.setActivity',
             {'type': 'typing', 'peer_id': peer_id}
         )
     except Exception as e:
         logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Å—Ç–∞—Ç—É—Å 'typing' –¥–ª—è peer_id={peer_id}: {e}")

# --- Silence Mode Management (Permanent Only) ---

async def save_silence_state_to_file():
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–æ—Å—Ç–æ—è–Ω–Ω—ã—Ö —Ä–µ–∂–∏–º–æ–≤ –º–æ–ª—á–∞–Ω–∏—è –≤ JSON-—Ñ–∞–π–ª."""
    logger.debug("–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø–æ—Å—Ç–æ—è–Ω–Ω—ã—Ö —Ä–µ–∂–∏–º–æ–≤ –º–æ–ª—á–∞–Ω–∏—è –≤ —Ñ–∞–π–ª...")
    data_to_save = {str(peer_id): True for peer_id in chat_silence_state if chat_silence_state[peer_id]}
    try:
        def _save():
            with open(SILENCE_STATE_FILE, "w", encoding="utf-8") as f:
                json.dump(data_to_save, f, indent=4)
        await asyncio.to_thread(_save)
        logger.info(f"–°–æ—Å—Ç–æ—è–Ω–∏–µ –ø–æ—Å—Ç–æ—è–Ω–Ω—ã—Ö —Ä–µ–∂–∏–º–æ–≤ –º–æ–ª—á–∞–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ {SILENCE_STATE_FILE}")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Ä–µ–∂–∏–º–æ–≤ –º–æ–ª—á–∞–Ω–∏—è: {e}", exc_info=True)

async def load_silence_state_from_file():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–æ—Å—Ç–æ—è–Ω–Ω—ã—Ö —Ä–µ–∂–∏–º–æ–≤ –º–æ–ª—á–∞–Ω–∏—è –∏–∑ JSON-—Ñ–∞–π–ª–∞ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ –±–æ—Ç–∞."""
    global chat_silence_state
    logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø–æ—Å—Ç–æ—è–Ω–Ω—ã—Ö —Ä–µ–∂–∏–º–æ–≤ –º–æ–ª—á–∞–Ω–∏—è –∏–∑ —Ñ–∞–π–ª–∞...")
    try:
        def _load():
            if not os.path.exists(SILENCE_STATE_FILE):
                logger.info(f"–§–∞–π–ª {SILENCE_STATE_FILE} –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É.")
                return None
            with open(SILENCE_STATE_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        
        loaded_data = await asyncio.to_thread(_load)

        if not loaded_data:
            return

        restored_count = 0
        for peer_id_str, should_be_silent in loaded_data.items():
            try:
                peer_id = int(peer_id_str)
                if should_be_silent:
                    chat_silence_state[peer_id] = True
                    logger.info(f"–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ—Å—Ç–æ—è–Ω–Ω—ã–π —Ä–µ–∂–∏–º –º–æ–ª—á–∞–Ω–∏—è –¥–ª—è peer_id={peer_id}")
                    restored_count += 1
            except (ValueError, KeyError) as e: # –î–æ–±–∞–≤–ª–µ–Ω KeyError
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø–∏—Å–∏ –¥–ª—è peer_id_str='{peer_id_str}': {e}", exc_info=True)
        
        if restored_count > 0:
            logger.info(f"–£—Å–ø–µ—à–Ω–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ {restored_count} —Å–æ—Å—Ç–æ—è–Ω–∏–π –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–≥–æ –º–æ–ª—á–∞–Ω–∏—è.")
        else:
            logger.info("–ê–∫—Ç–∏–≤–Ω—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–≥–æ –º–æ–ª—á–∞–Ω–∏—è –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")

    except FileNotFoundError:
        logger.info(f"–§–∞–π–ª {SILENCE_STATE_FILE} –Ω–µ –Ω–∞–π–¥–µ–Ω. –ó–∞–ø—É—Å–∫ —Å —á–∏—Å—Ç—ã–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º –º–æ–ª—á–∞–Ω–∏—è.")
    except json.JSONDecodeError:
        logger.error(f"–û—à–∏–±–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è JSON –∏–∑ —Ñ–∞–π–ª–∞ {SILENCE_STATE_FILE}. –í–æ–∑–º–æ–∂–Ω–æ, —Ñ–∞–π–ª –ø–æ–≤—Ä–µ–∂–¥–µ–Ω.")
    except Exception as e:
        logger.error(f"–ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Ä–µ–∂–∏–º–æ–≤ –º–æ–ª—á–∞–Ω–∏—è: {e}", exc_info=True)

async def silence_user(peer_id: int):
    """–ê–∫—Ç–∏–≤–∏—Ä—É–µ—Ç –ü–û–°–¢–û–Ø–ù–ù–´–ô —Ä–µ–∂–∏–º –º–æ–ª—á–∞–Ω–∏—è –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è/—á–∞—Ç–∞ (–æ–±—ã—á–Ω–æ –∏–∑-–∑–∞ CRM)."""
    if chat_silence_state.get(peer_id):
        logger.info(f"–ü–æ—Å—Ç–æ—è–Ω–Ω—ã–π —Ä–µ–∂–∏–º –º–æ–ª—á–∞–Ω–∏—è –¥–ª—è peer_id={peer_id} —É–∂–µ –±—ã–ª –∞–∫—Ç–∏–≤–µ–Ω.")
        return

    logger.info(f"–ê–∫—Ç–∏–≤–∞—Ü–∏—è –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞ –º–æ–ª—á–∞–Ω–∏—è –¥–ª—è peer_id={peer_id}.")
    chat_silence_state[peer_id] = True
    await save_silence_state_to_file()

async def unsilence_user(peer_id: int):
    """–î–µ–∞–∫—Ç–∏–≤–∏—Ä—É–µ—Ç –ü–û–°–¢–û–Ø–ù–ù–´–ô —Ä–µ–∂–∏–º –º–æ–ª—á–∞–Ω–∏—è –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è/—á–∞—Ç–∞ (–∫–æ–º–∞–Ω–¥–æ–π 'speak')."""
    if peer_id in chat_silence_state:
        logger.info(f"–†—É—á–Ω–∞—è –¥–µ–∞–∫—Ç–∏–≤–∞—Ü–∏—è (–∫–æ–º–∞–Ω–¥–æ–π speak) —Ä–µ–∂–∏–º–∞ –º–æ–ª—á–∞–Ω–∏—è –¥–ª—è peer_id={peer_id}.")
        chat_silence_state.pop(peer_id)
        await save_silence_state_to_file()
    else:
         logger.debug(f"–ü–æ–ø—ã—Ç–∫–∞ —Å–Ω—è—Ç—å –º–æ–ª—á–∞–Ω–∏–µ –¥–ª—è peer_id={peer_id}, –Ω–æ –±–æ—Ç –∏ —Ç–∞–∫ –±—ã–ª –∞–∫—Ç–∏–≤–µ–Ω (–ø–æ—Å—Ç–æ—è–Ω–Ω–æ–µ –º–æ–ª—á–∞–Ω–∏–µ –Ω–µ –±—ã–ª–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ).")

# --- –§—É–Ω–∫—Ü–∏–∏ –±—É—Ñ–µ—Ä–∏–∑–∞—Ü–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏–π ---
async def schedule_buffered_processing(peer_id: int, original_user_id: int):
    log_prefix = f"schedule_buffered_processing(peer:{peer_id}, user:{original_user_id}):"
    current_task = asyncio.current_task()
    try:
        logger.debug(f"{log_prefix} –û–∂–∏–¥–∞–Ω–∏–µ {MESSAGE_BUFFER_SECONDS} —Å–µ–∫—É–Ω–¥...")
        await asyncio.sleep(MESSAGE_BUFFER_SECONDS)
        task_in_dict = user_message_timers.get(peer_id)
        if task_in_dict is not current_task:
            logger.info(f"{log_prefix} –¢–∞–π–º–µ—Ä —Å—Ä–∞–±–æ—Ç–∞–ª, –Ω–æ –æ–Ω —É—Å—Ç–∞—Ä–µ–ª. –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–º–µ–Ω–µ–Ω–∞.")
            return
        if peer_id in user_message_timers:
            del user_message_timers[peer_id]
        logger.debug(f"{log_prefix} –¢–∞–π–º–µ—Ä —Å—Ä–∞–±–æ—Ç–∞–ª –∏ —É–¥–∞–ª–µ–Ω. –í—ã–∑–æ–≤ process_buffered_messages.")
        asyncio.create_task(process_buffered_messages(peer_id, original_user_id))
    except asyncio.CancelledError:
        logger.info(f"{log_prefix} –¢–∞–π–º–µ—Ä –æ—Ç–º–µ–Ω–µ–Ω.")
    except Exception as e:
        logger.error(f"{log_prefix} –û—à–∏–±–∫–∞ –≤ –∑–∞–¥–∞—á–µ —Ç–∞–π–º–µ—Ä–∞: {e}", exc_info=True)
        if peer_id in user_message_timers and user_message_timers.get(peer_id) is current_task:
            del user_message_timers[peer_id]

async def process_buffered_messages(peer_id: int, original_user_id: int):
    log_prefix = f"process_buffered_messages(peer:{peer_id}, user:{original_user_id}):"
    logger.debug(f"{log_prefix} –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±—É—Ñ–µ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π.")
    async with user_processing_locks[peer_id]:
        logger.debug(f"{log_prefix} –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –¥–ª—è peer_id={peer_id} –ø–æ–ª—É—á–µ–Ω–∞.")
        messages_to_process = pending_messages.pop(peer_id, [])
        if peer_id in user_message_timers:
            logger.warning(f"{log_prefix} –¢–∞–π–º–µ—Ä –¥–ª—è peer_id={peer_id} –≤—Å–µ –µ—â–µ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–ª! –û—Ç–º–µ–Ω—è–µ–º –∏ —É–¥–∞–ª—è–µ–º.")
            timer_to_cancel = user_message_timers.pop(peer_id)
            if not timer_to_cancel.done():
                try:
                    timer_to_cancel.cancel()
                except Exception as e_inner_cancel:
                    logger.debug(f"{log_prefix} –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ø—ã—Ç–∫–µ –æ—Ç–º–µ–Ω–∏—Ç—å —Ç–∞–π–º–µ—Ä: {e_inner_cancel}")
        if not messages_to_process:
            logger.info(f"{log_prefix} –ù–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–π –≤ –±—É—Ñ–µ—Ä–µ –¥–ª—è peer_id={peer_id}.")
            return
        combined_input = "\n".join(messages_to_process)
        num_messages = len(messages_to_process)
        logger.info(f'{log_prefix} –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è peer_id={peer_id} ({num_messages} —Å–æ–æ–±—â.): "{combined_input[:200]}..."')
        try:
            await set_typing_activity(peer_id)
            response_text = await chat_with_assistant(original_user_id, combined_input)
            await send_vk_message(peer_id, response_text)
            logger.info(f"{log_prefix} –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∏ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –æ—Ç–≤–µ—Ç –¥–ª—è peer_id={peer_id}.")
        except Exception as e:
            logger.error(f"{log_prefix} –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∏–ª–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è peer_id={peer_id}: {e}", exc_info=True)
            try:
                await send_vk_message(peer_id, "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.")
            except Exception as send_err_e:
                logger.error(f"{log_prefix} –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ peer_id={peer_id}: {send_err_e}")
        finally:
            logger.debug(f"{log_prefix} –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –¥–ª—è peer_id={peer_id} –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∞.")

# --- OpenAI Assistant Interaction ---
async def get_or_create_thread(user_id: int) -> Optional[str]: # –ò—Å–ø–æ–ª—å–∑—É–µ–º Optional
    user_key = get_user_key(user_id)
    if user_key in user_threads:
        thread_id = user_threads[user_key]
        try:
            await openai_client.beta.threads.messages.list(thread_id=thread_id, limit=1)
            logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ç—Ä–µ–¥ {thread_id} –¥–ª—è user_id={user_id}")
            return thread_id
        except openai.NotFoundError:
            logger.warning(f"–¢—Ä–µ–¥ {thread_id} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ OpenAI –¥–ª—è user_id={user_id}. –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π.")
            del user_threads[user_key] # –£–¥–∞–ª—è–µ–º –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π ID
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ —Ç—Ä–µ–¥—É {thread_id} –¥–ª—è user_id={user_id}: {e}. –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π.")
            if user_key in user_threads: # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–¥ —É–¥–∞–ª–µ–Ω–∏–µ–º
                 del user_threads[user_key]
    try:
        logger.info(f"–°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —Ç—Ä–µ–¥ –¥–ª—è user_id={user_id}...")
        thread = await openai_client.beta.threads.create()
        thread_id = thread.id
        user_threads[user_key] = thread_id
        logger.info(f"–°–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π —Ç—Ä–µ–¥ {thread_id} –¥–ª—è user_id={user_id}")
        return thread_id
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –Ω–æ–≤–æ–≥–æ —Ç—Ä–µ–¥–∞ –¥–ª—è user_id={user_id}: {e}", exc_info=True)
        return None

async def chat_with_assistant(user_id: int, message_text: str) -> str:
    thread_id = await get_or_create_thread(user_id)
    if not thread_id:
        return "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ (–Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Ç—Ä–µ–¥)."
    try:
        context = ""
        if vector_collection:
             context = await get_relevant_context(message_text, k=RELEVANT_CONTEXT_COUNT)
             await log_context(user_id, message_text, context)
        full_prompt = message_text
        if context:
            full_prompt = f"–ò—Å–ø–æ–ª—å–∑—É–π —Å–ª–µ–¥—É—é—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –¥–ª—è –æ—Ç–≤–µ—Ç–∞:\n--- –ù–ê–ß–ê–õ–û –ö–û–ù–¢–ï–ö–°–¢–ê ---\n{context}\n--- –ö–û–ù–ï–¶ –ö–û–ù–¢–ï–ö–°–¢–ê ---\n\n–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {message_text}"
        else:
            logger.info(f"–ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ user_id={user_id} –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –æ—Ç–∫–ª—é—á–µ–Ω–∞.")
        try:
            runs = await openai_client.beta.threads.runs.list(thread_id=thread_id)
            active_runs = [run for run in runs.data if run.status in ['queued', 'in_progress', 'requires_action']]
            if active_runs:
                logger.warning(f"–ù–∞–π–¥–µ–Ω—ã –∞–∫—Ç–∏–≤–Ω—ã–µ –∑–∞–ø—É—Å–∫–∏ ({len(active_runs)}) –¥–ª—è —Ç—Ä–µ–¥–∞ {thread_id}. –û—Ç–º–µ–Ω—è–µ–º...")
                for run in active_runs:
                    try:
                        await openai_client.beta.threads.runs.cancel(thread_id=thread_id, run_id=run.id)
                        logger.info(f"–û—Ç–º–µ–Ω–µ–Ω –∞–∫—Ç–∏–≤–Ω—ã–π –∑–∞–ø—É—Å–∫ {run.id} –¥–ª—è —Ç—Ä–µ–¥–∞ {thread_id}")
                    except Exception as cancel_error:
                        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–º–µ–Ω–∏—Ç—å –∑–∞–ø—É—Å–∫ {run.id}: {cancel_error}")
        except Exception as list_runs_error:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø—É—Å–∫–æ–≤ –¥–ª—è —Ç—Ä–µ–¥–∞ {thread_id}: {list_runs_error}")
        await openai_client.beta.threads.messages.create(
            thread_id=thread_id, role="user", content=full_prompt
        )
        logger.info(f"–°–æ–æ–±—â–µ–Ω–∏–µ –¥–æ–±–∞–≤–ª–µ–Ω–æ –≤ —Ç—Ä–µ–¥ {thread_id} –¥–ª—è user_id={user_id}")
        run = await openai_client.beta.threads.runs.create(
            thread_id=thread_id, assistant_id=ASSISTANT_ID
        )
        logger.info(f"–ó–∞–ø—É—â–µ–Ω –Ω–æ–≤—ã–π run {run.id} –¥–ª—è —Ç—Ä–µ–¥–∞ {thread_id}")
        start_time = time_module.time()
        while time_module.time() - start_time < OPENAI_RUN_TIMEOUT_SECONDS:
            await asyncio.sleep(1)
            run_status = await openai_client.beta.threads.runs.retrieve(
                thread_id=thread_id, run_id=run.id
            )
            if run_status.status == 'completed':
                logger.info(f"Run {run.id} —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω.")
                break
            elif run_status.status in ['failed', 'cancelled', 'expired']:
                error_message = f"Run {run.id} –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å–æ —Å—Ç–∞—Ç—É—Å–æ–º '{run_status.status}'."
                last_error = getattr(run_status, 'last_error', None)
                if last_error: error_message += f" –û—à–∏–±–∫–∞: {last_error.message} (–ö–æ–¥: {last_error.code})"
                logger.error(error_message)
                return "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ (—Å—Ç–∞—Ç—É—Å OpenAI)."
            elif run_status.status == 'requires_action':
                 logger.warning(f"Run {run.id} —Ç—Ä–µ–±—É–µ—Ç –¥–µ–π—Å—Ç–≤–∏—è (Function Calling?), —á—Ç–æ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è.")
                 await openai_client.beta.threads.runs.cancel(thread_id=thread_id, run_id=run.id)
                 return "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ (OpenAI requires_action)."
        else:
            logger.warning(f"–ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è ({OPENAI_RUN_TIMEOUT_SECONDS}s) –æ—Ç–≤–µ—Ç–∞ –æ—Ç OpenAI –¥–ª—è run {run.id}, —Ç—Ä–µ–¥ {thread_id}")
            try:
                await openai_client.beta.threads.runs.cancel(thread_id=thread_id, run_id=run.id)
                logger.info(f"–û—Ç–º–µ–Ω–µ–Ω run {run.id} –∏–∑-–∑–∞ —Ç–∞–π–º–∞—É—Ç–∞.")
            except Exception as cancel_error:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–º–µ–Ω–∏—Ç—å run {run.id} –ø–æ—Å–ª–µ —Ç–∞–π–º–∞—É—Ç–∞: {cancel_error}")
            return "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ (—Ç–∞–π–º–∞—É—Ç OpenAI)."
        messages_response = await openai_client.beta.threads.messages.list(
            thread_id=thread_id, order="desc", limit=5
        )
        assistant_response = None
        for msg in messages_response.data:
            if msg.role == "assistant" and msg.run_id == run.id:
                if msg.content and msg.content[0].type == 'text':
                    assistant_response = msg.content[0].text.value
                    logger.info(f"–ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –æ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –¥–ª—è user_id={user_id}: {assistant_response[:100]}...")
                    break
        if assistant_response:
            await log_context(user_id, message_text, context, assistant_response)
            return assistant_response
        else:
            logger.warning(f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –æ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –≤ —Ç—Ä–µ–¥–µ {thread_id} –ø–æ—Å–ª–µ run {run.id}. –û—Ç–≤–µ—Ç—ã: {messages_response.data}")
            for msg in messages_response.data:
                 if msg.role == "assistant":
                      if msg.content and msg.content[0].type == 'text':
                           logger.warning(f"–ù–∞–π–¥–µ–Ω –æ—Ç–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞, –Ω–æ –æ—Ç –¥—Ä—É–≥–æ–≥–æ run ({msg.run_id}) - –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ.")
                           return msg.content[0].text.value
            return "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ (–æ—Ç–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω)."
    except openai.APIError as e:
         logger.error(f"OpenAI API –æ—à–∏–±–∫–∞ –¥–ª—è user_id={user_id}: {e}", exc_info=True)
         return "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ (API OpenAI)."
    except Exception as e:
        logger.error(f"–ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –≤ chat_with_assistant –¥–ª—è user_id={user_id}: {e}", exc_info=True)
        return "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞."

# --- Vector Store Management (ChromaDB) ---
async def get_relevant_context(query: str, k: int) -> str:
    if not vector_collection:
        logger.warning("–ó–∞–ø—Ä–æ—Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –Ω–æ ChromaDB –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞.")
        return ""
    try:
        try:
            query_embedding_response = await openai_client.embeddings.create(
                 input=[query],
                 model=EMBEDDING_MODEL,
                 dimensions=EMBEDDING_DIMENSIONS if EMBEDDING_DIMENSIONS else None
            )
            query_embedding = query_embedding_response.data[0].embedding
            logger.debug(f"–≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ '{query[:50]}...' —Å–æ–∑–¥–∞–Ω.")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∑–∞–ø—Ä–æ—Å–∞: {e}", exc_info=True)
            return ""
        try:
            results = await asyncio.to_thread(
                vector_collection.query,
                query_embeddings=[query_embedding],
                n_results=k,
                include=["documents", "metadatas", "distances"]
            )
            logger.debug(f"–ü–æ–∏—Å–∫ –≤ ChromaDB –¥–ª—è '{query[:50]}...' –≤—ã–ø–æ–ª–Ω–µ–Ω.")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –ø–æ–∏—Å–∫–∞ –≤ ChromaDB: {e}", exc_info=True)
            return ""
        if not results or not results.get("ids") or not results["ids"][0]:
            logger.info(f"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: '{query[:50]}...'")
            return ""
        documents = results["documents"][0]
        metadatas = results["metadatas"][0]
        distances = results["distances"][0]
        context_pieces = []
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(documents)} –¥–æ–∫-–≤ –¥–ª—è '{query[:50]}...'. –¢–æ–ø {k}:")
        for i, (doc, meta, dist) in enumerate(zip(documents, metadatas, distances)):
            source = meta.get('source', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫')
            logger.info(f"  #{i+1}: –ò—Å—Ç–æ—á–Ω–∏–∫='{source}', –î–∏—Å—Ç–∞–Ω—Ü–∏—è={dist:.4f}, –ö–æ–Ω—Ç–µ–Ω—Ç='{doc[:100]}...'")
            context_piece = f"–ò–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ '{source}':\n{doc}"
            context_pieces.append(context_piece)
        if not context_pieces:
             logger.info(f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è '{query[:50]}...'.")
             return ""
        full_context = "\n\n---\n\n".join(context_pieces)
        logger.info(f"–°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ä–∞–∑–º–µ—Ä–æ–º {len(full_context)} —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ {len(context_pieces)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤.")
        return full_context
    except Exception as e:
        logger.error(f"–ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {e}", exc_info=True)
        return ""

async def update_vector_store():
    logger.info("--- –ó–∞–ø—É—Å–∫ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π ---")
    previous_active_subpath = _get_active_db_subpath()
    os.makedirs(VECTOR_DB_BASE_PATH, exist_ok=True)
    if not drive_service:
        logger.error("–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ë–ó –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ: —Å–µ—Ä–≤–∏—Å Google Drive –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω.")
        return {"success": False, "error": "–°–µ—Ä–≤–∏—Å Google Drive –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω", "added_chunks": 0, "total_chunks": 0}
    timestamp_dir_name = datetime.now().strftime("%Y%m%d_%H%M%S_%f") + "_new"
    new_db_path = os.path.join(VECTOR_DB_BASE_PATH, timestamp_dir_name)
    logger.info(f"–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –ë–î: {new_db_path}")
    try:
        os.makedirs(new_db_path, exist_ok=True)
    except Exception as e_mkdir:
        logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é '{new_db_path}': {e_mkdir}.", exc_info=True)
        return {"success": False, "error": f"Failed to create temp dir: {e_mkdir}", "added_chunks": 0, "total_chunks": 0}
    temp_vector_collection = None
    try:
        temp_chroma_client = chromadb.PersistentClient(path=new_db_path)
        temp_vector_collection = temp_chroma_client.get_or_create_collection(name=VECTOR_DB_COLLECTION_NAME)
        logger.info(f"–í—Ä–µ–º–µ–Ω–Ω–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è '{VECTOR_DB_COLLECTION_NAME}' —Å–æ–∑–¥–∞–Ω–∞/–ø–æ–ª—É—á–µ–Ω–∞ –≤ '{new_db_path}'.")
        logger.info("–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ Google Drive...")
        documents_data = await asyncio.to_thread(read_data_from_drive)
        if not documents_data:
            logger.warning("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ Google Drive. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ.")
            if os.path.exists(new_db_path): shutil.rmtree(new_db_path)
            return {"success": False, "error": "No documents in Google Drive", "added_chunks": 0, "total_chunks": 0}
        logger.info(f"–ü–æ–ª—É—á–µ–Ω–æ {len(documents_data)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ Google Drive.")
        all_texts, all_metadatas = [], []
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[("#", "h1"), ("##", "h2"), ("###", "h3")])
        MD_SECTION_MAX_LEN = 2000
        for doc_info in documents_data:
            doc_name, doc_content = doc_info['name'], doc_info['content']
            if not doc_content or not doc_content.strip():
                logger.warning(f"–î–æ–∫—É–º–µ–Ω—Ç '{doc_name}' –ø—É—Å—Ç. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.")
                continue
            enhanced_doc_content = f"–î–æ–∫—É–º–µ–Ω—Ç: {doc_name}\n\n{doc_content}"
            chunk_idx = 0
            is_md = doc_name.lower().endswith(('.md', '.markdown'))
            try:
                if is_md:
                    md_splits = markdown_splitter.split_text(enhanced_doc_content)
                    for md_split in md_splits:
                        headers_meta = {k: v for k, v in md_split.metadata.items() if k.startswith('h')}
                        if len(md_split.page_content) > MD_SECTION_MAX_LEN:
                            sub_chunks = text_splitter.split_text(md_split.page_content)
                            for sub_chunk_text in sub_chunks:
                                all_texts.append(sub_chunk_text)
                                all_metadatas.append({"source": doc_name, **headers_meta, "type": "md_split", "chunk": chunk_idx})
                                chunk_idx += 1
                        else:
                            all_texts.append(md_split.page_content)
                            all_metadatas.append({"source": doc_name, **headers_meta, "type": "md", "chunk": chunk_idx})
                            chunk_idx += 1
                else:
                    chunks = text_splitter.split_text(enhanced_doc_content)
                    for chunk_text in chunks:
                        all_texts.append(chunk_text)
                        all_metadatas.append({"source": doc_name, "type": "text", "chunk": chunk_idx})
                        chunk_idx += 1
                logger.info(f"–î–æ–∫—É–º–µ–Ω—Ç '{doc_name}' —Ä–∞–∑–±–∏—Ç –Ω–∞ {chunk_idx} —á–∞–Ω–∫–æ–≤.")
            except Exception as e_split:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–∑–±–∏–µ–Ω–∏–∏ '{doc_name}': {e_split}", exc_info=True)
                # Fallback to simple text splitting if markdown fails
                if is_md:
                    try:
                        chunks = text_splitter.split_text(enhanced_doc_content)
                        chunk_idx = 0
                        for chunk_text in chunks:
                            all_texts.append(chunk_text)
                            all_metadatas.append({"source": doc_name, "type": "text_fallback", "chunk": chunk_idx})
                            chunk_idx += 1
                        logger.info(f"–î–æ–∫—É–º–µ–Ω—Ç '{doc_name}' (fallback) —Ä–∞–∑–±–∏—Ç –Ω–∞ {chunk_idx} —á–∞–Ω–∫–æ–≤.")
                    except Exception as e_fallback:
                         logger.error(f"–û—à–∏–±–∫–∞ fallback-—Ä–∞–∑–±–∏–µ–Ω–∏—è '{doc_name}': {e_fallback}", exc_info=True)
                continue
        if not all_texts:
            logger.warning("–ù–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ –±–∞–∑—É. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ.")
            if os.path.exists(new_db_path): shutil.rmtree(new_db_path)
            return {"success": False, "error": "No text data to add", "added_chunks": 0, "total_chunks": 0}
        logger.info(f"–î–æ–±–∞–≤–ª–µ–Ω–∏–µ {len(all_texts)} —á–∞–Ω–∫–æ–≤ –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é...")
        try:
            all_ids = [f"{meta['source']}_{meta['chunk']}_{random.randint(1000,9999)}" for meta in all_metadatas] # Ensure unique IDs
            logger.info(f"–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(all_texts)} —á–∞–Ω–∫–æ–≤...")
            embeddings_response = await openai_client.embeddings.create(
                input=all_texts, model=EMBEDDING_MODEL,
                dimensions=EMBEDDING_DIMENSIONS if EMBEDDING_DIMENSIONS else None
            )
            all_embeddings = [item.embedding for item in embeddings_response.data]
            await asyncio.to_thread(
               temp_vector_collection.add, # type: ignore
               ids=all_ids, embeddings=all_embeddings, metadatas=all_metadatas, documents=all_texts
            )
            final_added, final_total = len(all_ids), temp_vector_collection.count() # type: ignore
            logger.info(f"–£—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω–æ {final_added} —á–∞–Ω–∫–æ–≤. –í—Å–µ–≥–æ: {final_total}.")
            active_db_info_filepath = os.path.join(VECTOR_DB_BASE_PATH, ACTIVE_DB_INFO_FILE)
            with open(active_db_info_filepath, "w", encoding="utf-8") as f: f.write(timestamp_dir_name)
            logger.info(f"–ü—É—Ç—å –∫ –Ω–æ–≤–æ–π –∞–∫—Ç–∏–≤–Ω–æ–π –±–∞–∑–µ '{timestamp_dir_name}' —Å–æ—Ö—Ä–∞–Ω–µ–Ω.")
            await _initialize_active_vector_collection()
            if not vector_collection:
                 return {"success": False, "error": "Failed to reload global vector_collection", "added_chunks": final_added, "total_chunks": final_total}
            if previous_active_subpath and previous_active_subpath != timestamp_dir_name:
                prev_path = os.path.join(VECTOR_DB_BASE_PATH, previous_active_subpath)
                if os.path.exists(prev_path):
                    shutil.rmtree(prev_path)
                    logger.info(f"–£–¥–∞–ª–µ–Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–∞—è –ë–î: '{prev_path}'")
            logger.info("--- –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ ---")
            return {"success": True, "added_chunks": final_added, "total_chunks": final_total, "new_active_path": timestamp_dir_name}
        except openai.APIError as e_openai:
             logger.error(f"OpenAI API –æ—à–∏–±–∫–∞ –ø—Ä–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞—Ö: {e_openai}", exc_info=True)
             if os.path.exists(new_db_path): shutil.rmtree(new_db_path)
             return {"success": False, "error": f"OpenAI API error: {e_openai}", "added_chunks": 0, "total_chunks": 0}
        except Exception as e_add:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –≤ ChromaDB: {e_add}", exc_info=True)
            if os.path.exists(new_db_path): shutil.rmtree(new_db_path)
            return {"success": False, "error": f"ChromaDB add error: {e_add}", "added_chunks": 0, "total_chunks": 0}
    except Exception as e_main_update:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ë–ó: {e_main_update}", exc_info=True)
        if os.path.exists(new_db_path): shutil.rmtree(new_db_path)
        return {"success": False, "error": f"Critical update error: {e_main_update}", "added_chunks": 0, "total_chunks": 0}

# --- Google Drive Reading ---
def read_data_from_drive() -> list[dict]:
    if not drive_service:
        logger.error("–ß—Ç–µ–Ω–∏–µ –∏–∑ Google Drive –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ: —Å–µ—Ä–≤–∏—Å –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω.")
        return []
    result_docs = []
    try:
        files_response = drive_service.files().list(
            q=f"'{FOLDER_ID}' in parents and trashed=false",
            fields="files(id, name, mimeType)", pageSize=1000
        ).execute()
        files = files_response.get('files', [])
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(files)} —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ Google Drive.")
        downloader_map = {
            'application/vnd.google-apps.document': download_google_doc,
            'application/pdf': download_pdf,
            'application/vnd.openxmlformats-officedocument.wordprocessingml.document': download_docx,
            'text/plain': download_text,
            'text/markdown': download_text, # Treat .md as plain text for download
        }
        for file_item in files: # Renamed to file_item to avoid conflict
            file_id, mime_type, file_name = file_item['id'], file_item['mimeType'], file_item['name']
            if mime_type in downloader_map:
                logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞: '{file_name}' (ID: {file_id}, Type: {mime_type})")
                try:
                    content = downloader_map[mime_type](drive_service, file_id)
                    if content and content.strip():
                        result_docs.append({'name': file_name, 'content': content})
                        logger.info(f"–£—Å–ø–µ—à–Ω–æ –ø—Ä–æ—á–∏—Ç–∞–Ω —Ñ–∞–π–ª: '{file_name}' ({len(content)} —Å–∏–º–≤)")
                    else:
                        logger.warning(f"–§–∞–π–ª '{file_name}' –ø—É—Å—Ç –∏–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∫–æ–Ω—Ç–µ–Ω—Ç.")
                except Exception as e_read_file:
                    logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ '{file_name}': {e_read_file}", exc_info=True)
            else:
                logger.debug(f"–§–∞–π–ª '{file_name}' –∏–º–µ–µ—Ç –Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø ({mime_type}).")
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ –∏–∑ Google Drive: {e}", exc_info=True)
        return []
    logger.info(f"–ß—Ç–µ–Ω–∏–µ –∏–∑ Google Drive –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –ü—Ä–æ—á–∏—Ç–∞–Ω–æ {len(result_docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.")
    return result_docs

def download_google_doc(service, file_id):
    request = service.files().export_media(fileId=file_id, mimeType='text/plain')
    fh = io.BytesIO()
    downloader = MediaIoBaseDownload(fh, request)
    done = False
    while not done:
        status, done = downloader.next_chunk()
        logger.debug(f"Google Doc Downloader: file_id={file_id}, status={status.progress() if status else 'N/A'}")
    return fh.getvalue().decode('utf-8', errors='ignore')

def download_pdf(service, file_id):
    request = service.files().get_media(fileId=file_id)
    fh = io.BytesIO()
    downloader = MediaIoBaseDownload(fh, request)
    done = False
    while not done:
        status, done = downloader.next_chunk()
        logger.debug(f"PDF Downloader: file_id={file_id}, status={status.progress() if status else 'N/A'}")
    fh.seek(0)
    try:
        pdf_reader = PyPDF2.PdfReader(fh)
        return "".join(page.extract_text() + "\n" for page in pdf_reader.pages if page.extract_text())
    except Exception as e:
         logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ PDF (ID: {file_id}): {e}", exc_info=True)
         return ""

def download_docx(service, file_id):
    request = service.files().get_media(fileId=file_id)
    fh = io.BytesIO()
    downloader = MediaIoBaseDownload(fh, request)
    done = False
    while not done:
        status, done = downloader.next_chunk()
        logger.debug(f"DOCX Downloader: file_id={file_id}, status={status.progress() if status else 'N/A'}")
    fh.seek(0)
    try:
        doc = docx.Document(fh)
        return "\n".join(paragraph.text for paragraph in doc.paragraphs if paragraph.text)
    except Exception as e:
         logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ DOCX (ID: {file_id}): {e}", exc_info=True)
         return ""

def download_text(service, file_id): # Handles both text/plain and text/markdown
    request = service.files().get_media(fileId=file_id)
    fh = io.BytesIO()
    MediaIoBaseDownload(fh, request).next_chunk() # Simplified
    try:
        return fh.getvalue().decode('utf-8')
    except UnicodeDecodeError:
         logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å {file_id} –∫–∞–∫ UTF-8, –ø—Ä–æ–±—É–µ–º cp1251.")
         try: return fh.getvalue().decode('cp1251', errors='ignore')
         except Exception as e:
              logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å {file_id}: {e}")
              return ""

# --- History and Context Management ---
async def log_context(user_id, message_text, context, response_text=None):
    try:
        ts = datetime.now()
        log_filename = os.path.join(LOGS_DIR, f"context_{user_id}_{ts.strftime('%Y%m%d_%H%M%S')}.log")
        with open(log_filename, "w", encoding="utf-8") as f:
            f.write(f"Timestamp: {ts.isoformat()}\nUser ID: {user_id}\n"
                    f"--- User Query ---\n{message_text}\n"
                    f"--- Retrieved Context ---\n{context or '–ö–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω.'}\n")
            if response_text: f.write(f"--- Assistant Response ---\n{response_text}\n")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è user_id={user_id}: {e}", exc_info=True)

async def cleanup_old_context_logs():
    logger.info("–ó–∞–ø—É—Å–∫ –æ—á–∏—Å—Ç–∫–∏ —Å—Ç–∞—Ä—ã—Ö –ª–æ–≥–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞...")
    count = 0
    try:
        cutoff = time_module.time() - LOG_RETENTION_SECONDS
        for filename in glob.glob(os.path.join(LOGS_DIR, "context_*.log")):
            try:
                if os.path.getmtime(filename) < cutoff:
                    os.remove(filename)
                    count += 1
            except Exception: continue
        logger.info(f"–û—á–∏—Å—Ç–∫–∞ –ª–æ–≥–æ–≤: —É–¥–∞–ª–µ–Ω–æ {count} —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö —Ñ–∞–π–ª–æ–≤." if count else "–£—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –ª–æ–≥–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ –ª–æ–≥–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {e}", exc_info=True)

# --- Background Cleanup Task ---
last_auto_update_date: Optional[datetime.date] = None

async def background_cleanup_task():
    global last_auto_update_date
    while True:
        await asyncio.sleep(3600) # –ö–∞–∂–¥—ã–π —á–∞—Å
        logger.info("–ó–∞–ø—É—Å–∫ –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–π —Ñ–æ–Ω–æ–≤–æ–π –∑–∞–¥–∞—á–∏...")
        try:
            now_local = datetime.now(TARGET_TZ)
            if now_local.hour == 4 and (last_auto_update_date is None or last_auto_update_date < now_local.date()):
                logger.info(f"–í—Ä–µ–º—è –¥–ª—è –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ë–ó ({now_local.hour}:00). –ó–∞–ø—É—Å–∫–∞–µ–º...")
                await run_update_and_notify_admin(ADMIN_USER_ID)
                last_auto_update_date = now_local.date()
        except Exception as e_auto_update:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ –ª–æ–≥–∏–∫–µ –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ë–ó: {e_auto_update}", exc_info=True)
        await cleanup_old_context_logs()
        logger.info("–ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è —Ñ–æ–Ω–æ–≤–∞—è –∑–∞–¥–∞—á–∞ –∑–∞–≤–µ—Ä—à–∏–ª–∞ —Ü–∏–∫–ª.")

# --- Main Event Handler ---
async def handle_new_message(event: VkBotEvent):
    global user_threads
    try:
        if event.from_user:
            user_id = event.obj.message['from_id']
            peer_id = event.obj.message['peer_id']
            message_text = event.obj.message['text'].strip()
            if not message_text:
                 logger.info(f"–ü—É—Å—Ç–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç user_id={user_id}. –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º.")
                 return

            if message_text.lower() == "/update" and user_id == ADMIN_USER_ID:
                logger.info(f"–ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä {user_id} –∏–Ω–∏—Ü–∏–∏—Ä–æ–≤–∞–ª –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ë–ó.")
                await send_vk_message(peer_id, "üîÑ –ó–∞–ø—É—Å–∫–∞—é –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π...")
                asyncio.create_task(run_update_and_notify_admin(peer_id)) 
                return
            
            if message_text.lower() == "/reset":
                user_key = get_user_key(user_id)
                log_prefix = f"handle_new_message(reset for peer:{peer_id}, user:{user_id}):"
                logging.info(f"{log_prefix} –ü–æ–ª—É—á–µ–Ω–∞ –∫–æ–º–∞–Ω–¥–∞ —Å–±—Ä–æ—Å–∞ –¥–∏–∞–ª–æ–≥–∞.")
                if peer_id in pending_messages: del pending_messages[peer_id]
                if peer_id in user_message_timers:
                    old_timer = user_message_timers.pop(peer_id)
                    if not old_timer.done(): old_timer.cancel()
                thread_id_to_forget = user_threads.pop(user_key, None)
                if thread_id_to_forget: logging.info(f"{log_prefix} –¢—Ä–µ–¥ {thread_id_to_forget} —É–¥–∞–ª–µ–Ω –∏–∑ –ø–∞–º—è—Ç–∏.")
                await send_vk_message(peer_id, "üîÑ –î–∏–∞–ª–æ–≥ —Å–±—Ä–æ—à–µ–Ω.")
                return
            
            if message_text.lower() == "/reset_all" and user_id == ADMIN_USER_ID:
                log_prefix = f"handle_new_message(reset_all from user:{user_id}):"
                logging.info(f"{log_prefix} –ü–æ–ª—É—á–µ–Ω–∞ –∫–æ–º–∞–Ω–¥–∞ —Å–±—Ä–æ—Å–∞ –í–°–ï–• –¥–∏–∞–ª–æ–≥–æ–≤.")
                active_timer_count = sum(1 for task in user_message_timers.values() if not task.done())
                for task in user_message_timers.values():
                    if not task.done(): task.cancel()
                user_message_timers.clear()
                pending_count = len(pending_messages)
                pending_messages.clear()
                threads_count = len(user_threads)
                user_threads.clear()
                await send_vk_message(peer_id, f"üîÑ –°–ë–†–û–° –í–°–ï–• –î–ò–ê–õ–û–ì–û–í –í–´–ü–û–õ–ù–ï–ù.\n- –¢–∞–π–º–µ—Ä–æ–≤: {active_timer_count}\n- –ë—É—Ñ–µ—Ä–æ–≤: {pending_count}\n- –¢—Ä–µ–¥–æ–≤: {threads_count}")
                return

            is_manager = user_id in MANAGER_USER_IDS or user_id == ADMIN_USER_ID
            if is_manager:
                command = message_text.lower()
                if command == CMD_SPEAK.lower(): # –¢–æ–ª—å–∫–æ –∫–æ–º–∞–Ω–¥–∞ speak
                    await unsilence_user(peer_id) # –°–Ω–∏–º–∞–µ–º –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–µ –º–æ–ª—á–∞–Ω–∏–µ
                    await send_vk_message(peer_id, "ü§ñ –†–µ–∂–∏–º –º–æ–ª—á–∞–Ω–∏—è —Å–Ω—è—Ç. –ë–æ—Ç —Å–Ω–æ–≤–∞ –∞–∫—Ç–∏–≤–µ–Ω.")
                    return

            if chat_silence_state.get(peer_id, False): # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–µ –º–æ–ª—á–∞–Ω–∏–µ
                logger.info(f"–ë–æ—Ç –≤ —Ä–µ–∂–∏–º–µ –º–æ–ª—á–∞–Ω–∏—è –¥–ª—è peer_id={peer_id} (CRM). –°–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç user_id={user_id} –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è.")
                return

            now_dt = datetime.now()
            last_time = user_last_message_time.get(user_id)
            if last_time and now_dt - last_time < timedelta(seconds=MESSAGE_COOLDOWN_SECONDS):
                logger.warning(f"–ö—É–ª–¥–∞—É–Ω –¥–ª—è user_id={user_id}. –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º.")
                return
            user_last_message_time[user_id] = now_dt

            logger.info(f"–ü–æ–ª—É—á–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç user_id={user_id} (peer_id={peer_id}): '{message_text[:100]}...'")
            pending_messages.setdefault(peer_id, []).append(message_text)
            logger.debug(f"–°–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç peer_id={peer_id} –¥–æ–±–∞–≤–ª–µ–Ω–æ –≤ –±—É—Ñ–µ—Ä: {pending_messages[peer_id]}")
            if peer_id in user_message_timers:
                old_timer = user_message_timers.pop(peer_id)
                if not old_timer.done():
                    try: old_timer.cancel()
                    except Exception as e_cancel: logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–º–µ–Ω–∏—Ç—å —Ç–∞–π–º–µ—Ä: {e_cancel}")
            logger.debug(f"–ó–∞–ø—É—Å–∫ —Ç–∞–π–º–µ—Ä–∞ –±—É—Ñ–µ—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è peer_id={peer_id} ({MESSAGE_BUFFER_SECONDS} —Å–µ–∫).")
            new_timer_task = asyncio.create_task(schedule_buffered_processing(peer_id, user_id))
            user_message_timers[peer_id] = new_timer_task
        
        elif event.from_chat:
            chat_id = event.chat_id # type: ignore
            user_id = event.obj.message['from_id']
            message_text = event.obj.message['text'].strip()
            logger.debug(f"–°–æ–æ–±—â–µ–Ω–∏–µ –≤ —á–∞—Ç–µ {chat_id} –æ—Ç {user_id}: {message_text[:50]}")
            # –õ–æ–≥–∏–∫–∞ –¥–ª—è –≥—Ä—É–ø–ø–æ–≤—ã—Ö —á–∞—Ç–æ–≤ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –±–æ—Ç–∞
            pass
        else:
            logger.debug(f"–ü–æ–ª—É—á–µ–Ω–æ —Å–æ–±—ã—Ç–∏–µ Long Poll —Ç–∏–ø–∞ {event.type}, –Ω–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è.")
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ handle_new_message: {e}", exc_info=True)

# --- Main Application Logic ---
async def run_update_and_notify_admin(notification_peer_id: int):
    logger.info(f"run_update_and_notify_admin: –ó–∞–ø—É—Å–∫ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ë–ó –¥–ª—è peer_id={notification_peer_id}")
    update_result = await update_vector_store()
    current_time_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    admin_message = f"üîî –û—Ç—á–µ—Ç –æ–± –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –ë–ó ({current_time_str}):\n"
    if update_result.get("success"):
        admin_message += (f"‚úÖ –£—Å–ø–µ—à–Ω–æ!\n‚ûï –î–æ–±–∞–≤–ª–µ–Ω–æ: {update_result.get('added_chunks', 'N/A')}\n"
                          f"üìä –í—Å–µ–≥–æ: {update_result.get('total_chunks', 'N/A')}\n")
        if update_result.get("new_active_path"): admin_message += f"üìÅ –ü—É—Ç—å: {update_result['new_active_path']}"
    else:
        admin_message += f"‚ùå –û—à–∏–±–∫–∞: {update_result.get('error', 'N/A')}\n–ë–∞–∑–∞ –º–æ–≥–ª–∞ –Ω–µ –∏–∑–º–µ–Ω–∏—Ç—å—Å—è."
    logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ë–ó: {admin_message}")
    try:
        await send_vk_message(notification_peer_id, admin_message)
        if ADMIN_USER_ID != 0 and notification_peer_id != ADMIN_USER_ID and ADMIN_USER_ID > 0:
            await send_vk_message(ADMIN_USER_ID, "[–ê–≤—Ç–æ] " + admin_message)
    except Exception as e_notify:
        logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –∞–¥–º–∏–Ω—É: {e_notify}", exc_info=True)

async def main():
    logger.info("--- –ó–∞–ø—É—Å–∫ VK –±–æ—Ç–∞ ---")
    await load_silence_state_from_file() # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –º–æ–ª—á–∞–Ω–∏—è
    await _initialize_active_vector_collection()
    logger.info("–ó–∞–ø—É—Å–∫ —Ñ–æ–Ω–æ–≤–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ë–ó –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ...")
    asyncio.create_task(run_update_and_notify_admin(ADMIN_USER_ID))
    cleanup_task = asyncio.create_task(background_cleanup_task())
    logger.info("–§–æ–Ω–æ–≤–∞—è –∑–∞–¥–∞—á–∞ –æ—á–∏—Å—Ç–∫–∏ –∑–∞–ø—É—â–µ–Ω–∞.")
    logger.warning("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –°–ò–ù–•–†–û–ù–ù–´–ô VkBotLongPoll. –≠—Ç–æ –ë–õ–û–ö–ò–†–£–ï–¢ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Ü–∏–∫–ª.")
    listen_task = None
    try:
        loop = asyncio.get_running_loop()
        listen_task = asyncio.create_task(asyncio.to_thread(run_longpoll_sync, loop), name="VKLongPollListener")
        if listen_task: await listen_task
    except vk_api.exceptions.ApiError as e:
        logger.critical(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ VK API –≤ Long Poll: {e}", exc_info=True)
    except Exception as e:
         logger.critical(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ –≥–ª–∞–≤–Ω–æ–º —Ü–∏–∫–ª–µ: {e}", exc_info=True)
    finally:
        logger.info("–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã —Ñ–æ–Ω–æ–≤—ã—Ö –∑–∞–¥–∞—á...")
        if 'cleanup_task' in locals() and cleanup_task: cleanup_task.cancel() # type: ignore
        if listen_task and not listen_task.done():
             listen_task.cancel()
             logger.warning("–ó–∞–ø—Ä–æ—à–µ–Ω–∞ –æ—Ç–º–µ–Ω–∞ –∑–∞–¥–∞—á–∏ Long Poll.")
        await asyncio.gather(
            cleanup_task if 'cleanup_task' in locals() and cleanup_task else asyncio.sleep(0), # type: ignore
            listen_task if listen_task else asyncio.sleep(0),
            return_exceptions=True
        )
        logger.info("--- –ë–æ—Ç –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω ---")

def run_longpoll_sync(async_loop: asyncio.AbstractEventLoop):
    logger.info("–ó–∞–ø—É—Å–∫ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ Long Poll –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ...")
    MAX_RECONNECT_ATTEMPTS, RECONNECT_DELAY_SECONDS = 5, 30
    current_attempts = 0
    global vk_session_api, VK_GROUP_ID # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –æ–Ω–∏ –¥–æ—Å—Ç—É–ø–Ω—ã
    
    while True:
        try:
            if not vk_session_api:
                logger.error("[Thread LongPoll] vk_session_api –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞.")
                time_module.sleep(RECONNECT_DELAY_SECONDS * 5)
                continue

            logger.info(f"[Thread LongPoll] –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è VkBotLongPoll (–ø–æ–ø—ã—Ç–∫–∞ {current_attempts + 1}).")
            current_longpoll = VkBotLongPoll(vk_session_api, VK_GROUP_ID) # type: ignore
            logger.info("[Thread LongPoll] VkBotLongPoll –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω.")
            current_attempts = 0
            logger.info("[Thread LongPoll] –ù–∞—á–∞–ª–æ –ø—Ä–æ—Å–ª—É—à–∏–≤–∞–Ω–∏—è —Å–æ–±—ã—Ç–∏–π...")
            for event in current_longpoll.listen():
                if event.type == VkBotEventType.MESSAGE_NEW:
                    asyncio.run_coroutine_threadsafe(handle_new_message(event), async_loop)
                elif event.type == VkBotEventType.MESSAGE_REPLY:
                    logger.debug(f"–ü–æ–ª—É—á–µ–Ω–æ MESSAGE_REPLY: {event.obj}")
                    try:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç –Ω–∞—à–µ–π –≥—Ä—É–ø–ø—ã (–∏—Å—Ö–æ–¥—è—â–µ–µ)
                        # –∏ from_id —ç—Ç–æ ID –Ω–∞—à–µ–π –≥—Ä—É–ø–ø—ã (–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π)
                        is_outgoing_from_group = (event.obj.get('out') == 1 and 
                                                  event.obj.get('from_id') == -int(VK_GROUP_ID)) # type: ignore
                        
                        if is_outgoing_from_group:
                            event_random_id = event.obj.get('random_id')
                            peer_id = event.obj.get('peer_id')

                            if event_random_id is not None and event_random_id in MY_PENDING_RANDOM_IDS:
                                MY_PENDING_RANDOM_IDS.remove(event_random_id)
                                logger.debug(f"MESSAGE_REPLY –æ—Ç –±–æ—Ç–∞ (random_id: {event_random_id}) –¥–ª—è peer_id={peer_id}. –£–¥–∞–ª–µ–Ω –∏–∑ MY_PENDING_RANDOM_IDS.")
                            else:
                                crm_message_text = event.obj.get('text', '') 
                                logger.info(f"MESSAGE_REPLY –æ—Ç CRM/–æ–ø–µ—Ä–∞—Ç–æ—Ä–∞ (—Ç–µ–∫—Å—Ç: '{crm_message_text[:50]}...', random_id: {event_random_id}) –¥–ª—è peer_id={peer_id}. –ê–∫—Ç–∏–≤–∏—Ä—É–µ–º –ü–û–°–¢–û–Ø–ù–ù–´–ô —Ä–µ–∂–∏–º –º–æ–ª—á–∞–Ω–∏—è.")
                                if peer_id:
                                    # –ê–∫—Ç–∏–≤–∏—Ä—É–µ–º –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–µ –º–æ–ª—á–∞–Ω–∏–µ
                                    asyncio.run_coroutine_threadsafe(silence_user(peer_id), async_loop)
                                else:
                                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å peer_id –∏–∑ MESSAGE_REPLY –¥–ª—è CRM: {event.obj}")
                        else:
                             logger.debug(f"–ü—Ä–æ–ø—É—Å–∫–∞–µ–º MESSAGE_REPLY (–Ω–µ –æ—Ç –Ω–∞—à–µ–π –≥—Ä—É–ø–ø—ã –∏–ª–∏ –Ω–µ –∏—Å—Ö–æ–¥—è—â–µ–µ): {event.obj}")
                    except Exception as e_reply_proc:
                        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ MESSAGE_REPLY: {e_reply_proc}", exc_info=True)
                        logger.debug(f"–û—à–∏–±–æ—á–Ω—ã–π MESSAGE_REPLY: {event.obj}")
                else:
                    logger.debug(f"–ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–æ–±—ã—Ç–∏–µ —Ç–∏–ø–∞ {event.type}")
            logger.warning("[Thread LongPoll] –¶–∏–∫–ª listen() –∑–∞–≤–µ—Ä—à–∏–ª—Å—è. –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫...")
            current_attempts = 0
            time_module.sleep(RECONNECT_DELAY_SECONDS)
        except (requests.exceptions.RequestException, vk_api.exceptions.VkApiError) as e_net: # –ë–æ–ª–µ–µ –æ–±—â–∏–µ —Å–µ—Ç–µ–≤—ã–µ –æ—à–∏–±–∫–∏
            logger.error(f"[Thread LongPoll] –û—à–∏–±–∫–∞ —Å–µ—Ç–∏/VK API: {e_net}", exc_info=True)
            current_attempts += 1
            if MAX_RECONNECT_ATTEMPTS > 0 and current_attempts >= MAX_RECONNECT_ATTEMPTS:
                logger.critical(f"[Thread LongPoll] –ü—Ä–µ–≤—ã—à–µ–Ω–æ –º–∞–∫—Å. –ø–æ–ø—ã—Ç–æ–∫ –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è. –û—Å—Ç–∞–Ω–æ–≤–∫–∞.")
                asyncio.run_coroutine_threadsafe(send_vk_message(ADMIN_USER_ID, "–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: VK Long Poll –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω."), async_loop) # type: ignore
                break
            logger.info(f"[Thread LongPoll] –ü–∞—É–∑–∞ {RECONNECT_DELAY_SECONDS}—Å –ø–µ—Ä–µ–¥ –ø–æ–ø—ã—Ç–∫–æ–π {current_attempts + 1}...")
            time_module.sleep(RECONNECT_DELAY_SECONDS)
        except Exception as e_fatal:
            logger.critical(f"[Thread LongPoll] –ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e_fatal}", exc_info=True)
            current_attempts += 1 # –°—á–∏—Ç–∞–µ–º –ø–æ–ø—ã—Ç–∫—É
            if MAX_RECONNECT_ATTEMPTS > 0 and current_attempts >= MAX_RECONNECT_ATTEMPTS:
                 logger.critical(f"[Thread LongPoll] –ü—Ä–µ–≤—ã—à–µ–Ω–æ –º–∞–∫—Å. –ø–æ–ø—ã—Ç–æ–∫ –ø–æ—Å–ª–µ –Ω–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–æ–π –æ—à–∏–±–∫–∏. –û—Å—Ç–∞–Ω–æ–≤–∫–∞.")
                 asyncio.run_coroutine_threadsafe(send_vk_message(ADMIN_USER_ID, "–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: VK Long Poll –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω (–Ω–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞)."), async_loop) # type: ignore
                 break
            logger.info(f"[Thread LongPoll] –ü–∞—É–∑–∞ {RECONNECT_DELAY_SECONDS * 2}—Å –ø–µ—Ä–µ–¥ –ø–æ–ø—ã—Ç–∫–æ–π {current_attempts + 1}...")
            time_module.sleep(RECONNECT_DELAY_SECONDS * 2)
    logger.info("[Thread LongPoll] –ü–æ—Ç–æ–∫ Long Poll –∑–∞–≤–µ—Ä—à–µ–Ω.")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("–ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª KeyboardInterrupt. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã...")
    except Exception as e:
         logger.critical(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ: {e}", exc_info=True)